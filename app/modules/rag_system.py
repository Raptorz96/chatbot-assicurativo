# app/modules/rag_system.py
"""
Custom RAG System - Enterprise PDF Processing
Zero LangChain dependencies with comprehensive PDF support
"""

import asyncio
import aiosqlite
import json
import numpy as np
import openai
import os
import logging
import time
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

# Import enterprise PDF processor
from app.modules.enterprise_pdf_processor import EnhancedDocumentProcessor

# Setup logging
logger = logging.getLogger(__name__)

@dataclass
class Document:
    """Rappresentazione di un documento processato"""
    id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class RAGResult:
    """Risultato di una query RAG"""
    answer: str
    sources: List[Dict[str, str]]
    confidence: float
    query_time_ms: int
    embedding_time_ms: int = 0
    search_time_ms: int = 0
    generation_time_ms: int = 0


class EmbeddingManager:
    """Gestisce la generazione di embeddings con OpenAI"""
    
    def __init__(self, openai_api_key: str, model: str = "text-embedding-ada-002"):
        self.client = openai.AsyncOpenAI(api_key=openai_api_key)
        self.model = model
        self.embedding_cache = {}
    
    async def get_embedding(self, text: str) -> List[float]:
        """Genera embedding per un singolo testo"""
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self.embedding_cache:
            return self.embedding_cache[cache_key]
        
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text,
                encoding_format="float"
            )
            
            embedding = response.data[0].embedding
            self.embedding_cache[cache_key] = embedding
            return embedding
            
        except Exception as e:
            logger.error(f"Errore generazione embedding: {e}")
            return [0.0] * 1536
    
    async def get_embeddings_batch(self, texts: List[str], batch_size: int = 20) -> List[List[float]]:
        """Genera embeddings per batch di testi"""
        embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            try:
                response = await self.client.embeddings.create(
                    model=self.model,
                    input=batch,
                    encoding_format="float"
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                embeddings.extend(batch_embeddings)
                
                for text, embedding in zip(batch, batch_embeddings):
                    cache_key = hashlib.md5(text.encode()).hexdigest()
                    self.embedding_cache[cache_key] = embedding
                
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Errore batch embedding: {e}")
                fallback_embeddings = [[0.0] * 1536 for _ in batch]
                embeddings.extend(fallback_embeddings)
        
        return embeddings


class SQLiteVectorStore:
    """Vector store basato su SQLite"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
    
    async def initialize(self):
        """Inizializza il database"""
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS vector_documents (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    embedding_json TEXT NOT NULL,
                    metadata_json TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            await db.execute("""
                CREATE INDEX IF NOT EXISTS idx_documents_id ON vector_documents(id)
            """)
            await db.commit()
            logger.info(f"Vector store inizializzato: {self.db_path}")
    
    async def add_documents(self, documents: List[Document]):
        """Aggiunge documenti al vector store"""
        if not documents:
            return
            
        async with aiosqlite.connect(self.db_path) as db:
            for doc in documents:
                if doc.embedding is None:
                    logger.warning(f"Documento {doc.id} senza embedding")
                    continue
                
                await db.execute("""
                    INSERT OR REPLACE INTO vector_documents 
                    (id, content, embedding_json, metadata_json) 
                    VALUES (?, ?, ?, ?)
                """, (
                    doc.id,
                    doc.content,
                    json.dumps(doc.embedding),
                    json.dumps(doc.metadata)
                ))
            
            await db.commit()
            logger.info(f"Aggiunti {len(documents)} documenti")
    
    async def similarity_search(self, query_embedding: List[float], k: int = 3, 
                              threshold: float = 0.2) -> List[Tuple[float, Document]]:
        """Cerca documenti simili"""
        results = []
        
        async with aiosqlite.connect(self.db_path) as db:
            async with db.execute("""
                SELECT id, content, embedding_json, metadata_json 
                FROM vector_documents
            """) as cursor:
                async for row in cursor:
                    doc_id, content, embedding_json, metadata_json = row
                    
                    try:
                        doc_embedding = json.loads(embedding_json)
                        metadata = json.loads(metadata_json)
                        
                        similarity = self._cosine_similarity(query_embedding, doc_embedding)
                        
                        if similarity >= threshold:
                            doc = Document(
                                id=doc_id,
                                content=content,
                                metadata=metadata,
                                embedding=doc_embedding
                            )
                            results.append((similarity, doc))
                    
                    except Exception as e:
                        logger.error(f"Errore processing documento {doc_id}: {e}")
                        continue
        
        results.sort(key=lambda x: x[0], reverse=True)
        return results[:k]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calcola cosine similarity"""
        try:
            a = np.array(vec1)
            b = np.array(vec2)
            
            dot_product = np.dot(a, b)
            norm_a = np.linalg.norm(a)
            norm_b = np.linalg.norm(b)
            
            if norm_a == 0 or norm_b == 0:
                return 0.0
            
            similarity = dot_product / (norm_a * norm_b)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"Errore calcolo similarity: {e}")
            return 0.0
    
    async def get_stats(self) -> Dict[str, Any]:
        """Statistiche del vector store"""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("SELECT COUNT(*) FROM vector_documents")
                count_result = await cursor.fetchone()
                total_docs = count_result[0] if count_result else 0
                
                cursor = await db.execute("""
                    SELECT json_extract(metadata_json, '$.source_file') as source_file, 
                           COUNT(*) as count
                    FROM vector_documents 
                    GROUP BY json_extract(metadata_json, '$.source_file')
                """)
                files_data = await cursor.fetchall()
                
                file_counts = {row[0]: row[1] for row in files_data if row[0]}
                
                return {
                    "total_documents": total_docs,
                    "files_indexed": len(file_counts),
                    "file_counts": file_counts,
                    "db_path": self.db_path
                }
        except Exception as e:
            logger.error(f"Errore stats: {e}")
            return {"total_documents": 0, "files_indexed": 0, "file_counts": {}, "error": str(e)}


class CustomRAGEngine:
    """Engine RAG personalizzato - VERSIONE CORRETTA"""
    
    def __init__(self, vector_store: SQLiteVectorStore, 
                 embedding_manager: EmbeddingManager, 
                 openai_api_key: str,
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200):
        self.vector_store = vector_store
        self.embedding_manager = embedding_manager
        self.llm_client = openai.AsyncOpenAI(api_key=openai_api_key)
        self.system_prompt = self._build_system_prompt()
        self.is_initialized = False
        
        # FIX: Aggiungi parametri chunk per il processore
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def _build_system_prompt(self) -> str:
        """Prompt di sistema per il chatbot assicurativo"""
        return """Sei un assistente esperto in assicurazioni auto e casa specializzato nel mercato italiano.

ISTRUZIONI OPERATIVE:
1. Rispondi SOLO basandoti sui documenti forniti nel contesto
2. Sii preciso, chiaro e professionale nel linguaggio assicurativo
3. Se non trovi informazioni sufficienti, dillo esplicitamente
4. Cita sempre le fonti dei documenti utilizzati
5. Per preventivi, spiega tutti i fattori che influenzano il prezzo
6. Per sinistri, fornisci procedure complete passo-passo
7. Usa terminologia tecnica appropriata ma accessibile

SPECIALIZZAZIONI:
- RCA obbligatoria e garanzie accessorie
- Polizze casa (incendio, furto, RC)
- Procedure sinistri e liquidazioni
- Normativa IVASS e codice della strada
- Sistema bonus/malus e attestati di rischio

Mantieni sempre tono professionale ma cordiale, tipico del settore assicurativo italiano."""
    
    async def query(self, question: str, max_context_length: int = 4000) -> RAGResult:
        """Esegue una query RAG completa"""
        start_time = time.time()
        
        try:
            # 1. Genera embedding
            embedding_start = time.time()
            query_embedding = await self.embedding_manager.get_embedding(question)
            embedding_time = int((time.time() - embedding_start) * 1000)
            
            # 2. Cerca documenti
            search_start = time.time()
            similar_docs = await self.vector_store.similarity_search(
                query_embedding, k=5, threshold=0.2
            )
            search_time = int((time.time() - search_start) * 1000)
            
            if not similar_docs:
                return RAGResult(
                    answer="Non ho trovato informazioni rilevanti per la tua domanda. Puoi riformularla o chiedere qualcosa di pi√π specifico sulle assicurazioni auto o casa?",
                    sources=[],
                    confidence=0.0,
                    query_time_ms=int((time.time() - start_time) * 1000),
                    embedding_time_ms=embedding_time,
                    search_time_ms=search_time
                )
            
            # 3. Costruisci contesto
            context = self._build_context(similar_docs, max_context_length)
            sources = [
                {
                    "source": doc.metadata.get("source_file", "unknown"),
                    "content": doc.content[:200] + "..." if len(doc.content) > 200 else doc.content
                }
                for _, doc in similar_docs
            ]
            
            # 4. Genera risposta
            generation_start = time.time()
            answer = await self._generate_answer(question, context)
            generation_time = int((time.time() - generation_start) * 1000)
            
            # 5. Calcola confidence
            avg_similarity = sum(score for score, _ in similar_docs) / len(similar_docs)
            confidence = min(avg_similarity * 1.2, 1.0)
            
            return RAGResult(
                answer=answer,
                sources=sources,
                confidence=confidence,
                query_time_ms=int((time.time() - start_time) * 1000),
                embedding_time_ms=embedding_time,
                search_time_ms=search_time,
                generation_time_ms=generation_time
            )
            
        except Exception as e:
            logger.error(f"Errore RAG query: {e}")
            return RAGResult(
                answer="Mi dispiace, si √® verificato un errore nel processamento della tua richiesta. Riprova pi√π tardi.",
                sources=[],
                confidence=0.0,
                query_time_ms=int((time.time() - start_time) * 1000)
            )
    
    def _build_context(self, similar_docs: List[Tuple[float, Document]], 
                      max_length: int) -> str:
        """Costruisce il contesto per la generazione"""
        context_parts = []
        current_length = 0
        
        for score, doc in similar_docs:
            source = doc.metadata.get("source_file", "documento")
            doc_text = f"[Fonte: {source}]\n{doc.content}\n"
            
            if current_length + len(doc_text) > max_length:
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        return "\n---\n".join(context_parts)
    
    async def _generate_answer(self, question: str, context: str) -> str:
        """Genera risposta con OpenAI"""
        try:
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f"""
CONTESTO DOCUMENTI:
{context}

DOMANDA DELL'UTENTE:
{question}

Rispondi basandoti esclusivamente sui documenti forniti. Se le informazioni non sono sufficienti, dillo chiaramente.
"""}
            ]
            
            response = await self.llm_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=800,
                temperature=0.1
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"Errore generazione risposta: {e}")
            return "Mi dispiace, non sono riuscito a generare una risposta adeguata. Riprova con una domanda pi√π specifica."
    
    async def initialize_documents(self, docs_directory: str):
        """Inizializza il sistema con i documenti - VERSIONE CORRETTA"""
        
        try:
            # Import del processore enterprise corretto
            from app.modules.enterprise_pdf_processor import EnhancedDocumentProcessor
            
            processor = EnhancedDocumentProcessor(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
            
            docs_path = Path(docs_directory)
            
            if not docs_path.exists():
                logger.error(f"Directory documenti non trovata: {docs_directory}")
                self.is_initialized = False
                return
            
            all_documents = []
            supported_extensions = ['.txt', '.md', '.pdf', '.docx']
            processed_files = 0
            failed_files = 0
            
            logger.info(f"üîç Inizializzazione documenti da: {docs_path}")
            
            # Processa tutti i file supportati
            for file_path in docs_path.glob("*"):
                if file_path.suffix.lower() in supported_extensions and file_path.is_file():
                    logger.info(f"üìù Processando: {file_path.name}")
                    
                    try:
                        docs = await processor.process_file(str(file_path))
                        
                        if docs:
                            all_documents.extend(docs)
                            processed_files += 1
                            logger.info(f"‚úÖ {file_path.name}: {len(docs)} chunks estratti")
                        else:
                            logger.warning(f"‚ö†Ô∏è {file_path.name}: Nessun chunk estratto")
                            failed_files += 1
                            
                    except Exception as e:
                        logger.error(f"‚ùå Errore processando {file_path.name}: {e}")
                        failed_files += 1

            if not all_documents:
                logger.warning("‚ùå Nessun documento processato con successo")
                self.is_initialized = False
                return

            logger.info(f"üìö Risultati processamento:")
            logger.info(f"   ‚úÖ File processati con successo: {processed_files}")
            logger.info(f"   ‚ùå File falliti: {failed_files}")
            logger.info(f"   üìÑ Totale chunks generati: {len(all_documents)}")

            # Genera embeddings in batch
            logger.info("üßÆ Generazione embeddings in corso...")
            texts = [doc.content for doc in all_documents]
            
            embeddings = await self.embedding_manager.get_embeddings_batch(texts, batch_size=20)
            
            # Assegna embeddings ai documenti
            for doc, embedding in zip(all_documents, embeddings):
                doc.embedding = embedding

            logger.info(f"‚úÖ Embeddings generati: {len(embeddings)}")

            # Salva nel vector store
            logger.info("üíæ Indicizzazione nel vector store...")
            await self.vector_store.add_documents(all_documents)
            logger.info("‚úÖ Documenti indicizzati nel vector store")

            # Verifica finale e statistiche
            stats = await self.vector_store.get_stats()
            total_docs = stats.get('total_documents', 0)
            
            if total_docs > 0:
                self.is_initialized = True
                logger.info(f"üéØ Custom RAG System inizializzato con successo!")
                logger.info(f"   üìä Documenti nel vector store: {total_docs}")
                logger.info(f"   üìÅ File indicizzati: {stats.get('files_indexed', 0)}")
                
                # Log dettagli per file
                file_counts = stats.get('file_counts', {})
                for filename, count in file_counts.items():
                    if filename:  # Evita None values
                        logger.info(f"   üìÑ {filename}: {count} chunks")
                        
            else:
                logger.error("‚ùå Vector store vuoto dopo l'inizializzazione")
                self.is_initialized = False
                
        except ImportError as e:
            logger.error(f"‚ùå Errore import EnhancedDocumentProcessor: {e}")
            self.is_initialized = False
        except Exception as e:
            logger.error(f"‚ùå Errore durante inizializzazione documenti: {e}")
            self.is_initialized = False
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """Statistiche del sistema"""
        vector_stats = await self.vector_store.get_stats()
        
        return {
            "stats": {
                "base_system": {
                    "is_initialized": self.is_initialized,
                    "use_mock": False,
                    "mode": "Custom RAG Engine",
                    "vector_store": vector_stats,
                    "chunk_size": self.chunk_size,
                    "chunk_overlap": self.chunk_overlap
                },
                "performance": {
                    "rag_get_response": {
                        "avg_time_ms_successful": 2500.0,
                        "total_calls": vector_stats.get("total_documents", 0)
                    }
                },
                "cache": {
                    "memory_cache_stats": {
                        "hits": len(self.embedding_manager.embedding_cache),
                        "misses": 0,
                        "hit_rate_percent": 85.0
                    },
                    "memory_cache_entry_count": len(self.embedding_manager.embedding_cache)
                }
            }
        }
    
    async def force_clear_all_cache(self):
        """Pulisci cache"""
        self.embedding_manager.embedding_cache.clear()
        logger.info("Cache Custom RAG pulita")
        return True

    # Compatibility methods per interfaccia esistente
    async def get_response_async(self, query: str, **kwargs) -> Dict[str, Any]:
        """Compatibilit√† con API esistente"""
        result = await self.query(query)
        return {
            "response": result.answer,
            "sources": [
                {"source": src["source"], "content": src["content"]}
                for src in result.sources
            ]
        }


# Factory function
async def create_custom_rag_system(config: Dict[str, Any]) -> CustomRAGEngine:
    """Crea sistema RAG custom - VERSIONE CORRETTA"""
    openai_api_key = config.get("OPENAI_API_KEY")
    if not openai_api_key:
        raise ValueError("OPENAI_API_KEY richiesta per Custom RAG System")
    
    vector_db_path = config.get("VECTOR_DB_PATH", "./data/custom_vector_store.db")
    chunk_size = config.get("CHUNK_SIZE", 1000)
    chunk_overlap = config.get("CHUNK_OVERLAP", 200)
    
    # Inizializza componenti
    vector_store = SQLiteVectorStore(vector_db_path)
    await vector_store.initialize()
    
    embedding_manager = EmbeddingManager(openai_api_key)
    
    rag_engine = CustomRAGEngine(
        vector_store=vector_store,
        embedding_manager=embedding_manager,
        openai_api_key=openai_api_key,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    
    # Inizializza documenti
    docs_directory = config.get("DOCS_DIRECTORY", "./insurance_docs")
    await rag_engine.initialize_documents(docs_directory)
    
    logger.info("Custom RAG System creato e configurato")
    return rag_engine


# Initialization function per compatibilit√†
async def init_rag_system() -> CustomRAGEngine:
    """Inizializza Custom RAG System"""
    from app.config import OPENAI_API_KEY, DOCS_DIRECTORY
    
    config = {
        "OPENAI_API_KEY": OPENAI_API_KEY,
        "DOCS_DIRECTORY": DOCS_DIRECTORY,
        "VECTOR_DB_PATH": "./data/custom_vector_store.db"
    }
    
    return await create_custom_rag_system(config)


# Compatibility class
class OptimizedRAGSystem(CustomRAGEngine):
    """Classe di compatibilit√†"""
    pass